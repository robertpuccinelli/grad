{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('venv_PH240C')",
   "display_name": "Python 3.8.5 64-bit ('venv_PH240C')",
   "metadata": {
    "interpreter": {
     "hash": "cf9a760fe3c2c7ff646ff0602b42b3e83696fba07421404abe07dd1c961bb1de"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def rmMissing(df: pd.DataFrame, header: list):\n",
    "    idx = []\n",
    "    for i in range(1,len(df.columns)):\n",
    "        if (df[df.columns[i]].dtypes == \"float64\") or (sum(df[df.columns[i]]>1000)):\n",
    "            if len(df.loc[df[df.columns[i]]==0]):\n",
    "                idx.append(i)\n",
    "    for col in df[df.columns[idx]].columns:\n",
    "        del df[col]\n",
    "        headers.remove(col)\n",
    "\n",
    "df = pd.read_csv(\"dat.csv\")\n",
    "del df[\"chip\"]\n",
    "headers = [\"cad\", \"alcohol_intake\", \"fish_oil_intake\", \"close_to_road\", \"chest_pain\", \"high_bp\", \"high_chol\", \"diabetes\", \"location\", \"bmi\", \"age\", \"gender\", \"pca1\", \"pca2\", \"pca3\", \"walking_freq\", \"time_tv\", \"time_sleep\", \"var_sleep\", \"townsend\", \"veggie_intake\", \"bread_intake\", \"tea_intake\", \"dia_pressure\", \"sys_pressure\", \"pulse\", \"num_veh\", \"inv_maj_road_dist\", \"inv_road_dist\", \"no2_2005\", \"no2_2006\", \"no2_2007\", \"no2_2010\", \"no_2010\", \"pm10_2007\", \"len_maj_road\", \"traffic_load_man\",\"traffic_int_maj\", \"traffic_int_road\",\"day_noise\", \"night_noise\"]\n",
    "df.columns = headers\n",
    "rmMissing(df, headers)\n",
    "x = df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Full tree with pruning\"\"\"\n",
    "mode = \"Entropy\"\n",
    "name_modifier = \"Balanced\"#\", Full Set of Covariates\" #\n",
    "bal = \"balanced\" # None\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, df.iloc[:,0], random_state=2)\n",
    "entropy_tree = DecisionTreeClassifier(criterion=mode.lower(), class_weight=bal)\n",
    "et_path = entropy_tree.cost_complexity_pruning_path(X_train, Y_train)\n",
    "ccp_alphas, impurities = et_path.ccp_alphas, et_path.impurities\n",
    "\n",
    "fig, ax = plt.subplots(3,1, figsize=(7,8))\n",
    "ax[0].plot(ccp_alphas[:], impurities[:], marker='o', drawstyle=\"steps-post\", markerfacecolor=\"r\", color=\"k\")\n",
    "ax[0].set_xlabel(r\"$\\alpha$\")\n",
    "ax[0].set_ylabel(\"Total Impurity of Leaves\")\n",
    "ax[0].set_title(\"Total Impurity vs \"+ r'$\\alpha$' +\", \" + mode + name_modifier)\n",
    "\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha, criterion=mode.lower(), class_weight=bal)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    clfs.append(clf)\n",
    "node_counts = [clf.tree_.node_count for clf in clfs]\n",
    "depth = [clf.tree_.max_depth for clf in clfs]\n",
    "\n",
    "ax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\", markerfacecolor=\"r\",color=\"k\")\n",
    "ax[1].set_xlabel(r'$\\alpha$')\n",
    "ax[1].set_ylabel(\"Depth of Tree\")\n",
    "ax[1].set_title(\"Depth vs \" + r'$\\alpha$' + \", \" + mode + name_modifier)\n",
    "\n",
    "train_scores = [clf.score(X_train, Y_train) for clf in clfs]\n",
    "test_scores = [clf.score(X_test, Y_test) for clf in clfs]\n",
    "\n",
    "ax[2].set_xlabel(r'$\\alpha$')\n",
    "ax[2].set_ylabel(\"Accuracy\")\n",
    "ax[2].set_title(\"Accuracy vs \" + r'$\\alpha$' +\", \" + mode + name_modifier)\n",
    "ax[2].plot(ccp_alphas, train_scores, marker='o', label=\"Train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax[2].plot(ccp_alphas, test_scores, marker='o', label=\"Test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax[2].legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = []\n",
    "weighted_bias = []\n",
    "true_pos_corr = []\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(len(ccp_alphas)):\n",
    "    cm.append(confusion_matrix(clfs[i].predict(X_test), Y_test))\n",
    "    weighted_bias.append(((cm[i][1][0] + cm[i][1][1]) - (cm[i][0][1] + cm[i][1][1]))/((cm[i][0][1] + cm[i][1][1])))\n",
    "    true_pos_corr.append(cm[i][1][1] / (cm[i][0][1] + cm[i][1][1]))\n",
    "    plt.text(true_pos_corr[i],weighted_bias[i], \"%.4G\" % ccp_alphas[i])\n",
    "\n",
    "\n",
    "ax.plot(true_pos_corr,weighted_bias,  marker='o', drawstyle=\"steps-post\", markerfacecolor=\"r\",color=\"k\")\n",
    "ax.set_xlabel('True Positive Accuracy')\n",
    "ax.set_ylabel(\"Bias\")\n",
    "ax.set_title(\"Bias vs True Positive Accuracy, \" + mode + name_modifier)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_confusion_matrix(clfs[11],X_test, Y_test)\n",
    "disp.ax_.set_title(\"Confusion Matrix, \"+mode+\", \" + r'$\\alpha$' + '=' + str(ccp_alphas[11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydotplus.graphviz import graph_from_dot_data\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "dot_data = export_graphviz(                           # Create dot data\n",
    "    tree, filled=True, rounded=True,\n",
    "    class_names=['nCAD','CAD'],\n",
    "    feature_names=headers[1:],\n",
    "    out_file=None\n",
    ")\n",
    "\n",
    "graph = graph_from_dot_data(dot_data) \n",
    "graph.write_png('tree.png')                           # Write graph to PNG image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [k/100 for k in range(0,24, 1)]\n",
    "for i in range(1,51,2):\n",
    "    bcm = []\n",
    "    bweighted_bias = []\n",
    "    btrue_pos_corr = []\n",
    "    fig, ax = plt.subplots()\n",
    "    print(i)\n",
    "    for j in l:\n",
    "        bt = DecisionTreeClassifier(random_state=0, max_features=None, ccp_alpha=j, criterion=mode.lower(), class_weight=bal)\n",
    "        bag = BaggingClassifier(base_estimator=bt, n_estimators=i, max_samples=1.0)\n",
    "        bag = bag.fit(x, df.iloc[:,0])\n",
    "        bcm = confusion_matrix(bag.predict(x),  df.iloc[:,0] )\n",
    "        bweighted_bias = (((bcm[1][0] + bcm[1][1]) - (bcm[0][1] + bcm[1][1]))/((bcm[0][1] + bcm[1][1])))\n",
    "        btrue_pos_corr = (bcm[1][1] / (bcm[0][1] + bcm[1][1]))\n",
    "        plt.text(btrue_pos_corr,bweighted_bias, \"%.3G\" % j)\n",
    "\n",
    "    ax.plot(btrue_pos_corr,bweighted_bias,  marker='o', drawstyle=\"steps-post\", markerfacecolor=\"r\",color=\"k\")\n",
    "    ax.set_xlabel('True Positive Accuracy')\n",
    "    ax.set_ylabel(\"Bias\")\n",
    "    ax.set_title(\"Bias vs True Positive Accuracy, \" + mode + ', Num Estimators=' + str(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(bag.predict(x),  df.iloc[:,0] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}